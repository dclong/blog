Status: published
Date: 2019-12-24 18:39:31
Author: Benjamin Du
Slug: optimization-method-in-machine-learning
Title: Optimization Method in Machine Learning
Category: AI
Tags: AI, machine learning, data science, optimization

**
Things on this page are fragmentary and immature notes/thoughts of the author.
It is not meant to readers but rather for convenient reference of the author and future improvement.
**

Empirically, 
we observed that L-BFGS converges faster and with better solutions on small datasets. 
For relatively large datasets, however, Adam is very robust. 
It usually converges quickly and gives pretty good performance. 
SGD with momentum or nesterovâ€™s momentum, on the other hand, 
can perform better than those two algorithms if learning rate is correctly tuned.


