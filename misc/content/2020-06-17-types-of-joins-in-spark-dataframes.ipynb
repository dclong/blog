{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-06-17 15:24:04\n",
    "- Title: Types of Joins of Spark DataFrames\n",
    "- Slug: spark-dataframe-types-joins\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, Spark, DataFrame, join, type, inner join, outer join, left join, right join, full join, big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "1. it is suggested that you always use `Seq(c1, c2, ...)` even if there's only one field for joining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f46eaed-57ab-4764-b03b-869498b20dba",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.11 2.3.1\n",
    "org.apache.spark spark-sql_2.11 2.3.1\n",
    "org.apache.spark spark-hive_2.11 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@4a82094b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Types of Joins in Spark\")\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@7e391d71"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|  LastName|DepartmentID|\n",
      "+----------+------------+\n",
      "|  Rafferty|          31|\n",
      "|     Jones|          33|\n",
      "|Heisenberg|          33|\n",
      "|  Robinson|          34|\n",
      "|     Smith|          34|\n",
      "|       Ben|          50|\n",
      "|  Williams|        null|\n",
      "+----------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = Seq(\n",
    "  (\"Rafferty\", Some(31)), \n",
    "  (\"Jones\", Some(33)), \n",
    "  (\"Heisenberg\", Some(33)), \n",
    "  (\"Robinson\", Some(34)), \n",
    "  (\"Smith\", Some(34)), \n",
    "  (\"Ben\", Some(50)),\n",
    "  (\"Williams\", null)\n",
    ").toDF(\"LastName\", \"DepartmentID\")\n",
    "\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|DepartmentID|DepartmentName|\n",
      "+------------+--------------+\n",
      "|          31|         Sales|\n",
      "|          33|   Engineering|\n",
      "|          34|      Clerical|\n",
      "|          35|     Marketing|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val departments = Seq(\n",
    "  (31, \"Sales\"), \n",
    "  (33, \"Engineering\"), \n",
    "  (34, \"Clerical\"),\n",
    "  (35, \"Marketing\")\n",
    ").toDF(\"DepartmentID\", \"DepartmentName\")\n",
    "departments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+\n",
      "|DepartmentID|  LastName|DepartmentName|\n",
      "+------------+----------+--------------+\n",
      "|          31|  Rafferty|         Sales|\n",
      "|          33|     Jones|   Engineering|\n",
      "|          33|Heisenberg|   Engineering|\n",
      "|          34|  Robinson|      Clerical|\n",
      "|          34|     Smith|      Clerical|\n",
      "+------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Outer Join\n",
    "\n",
    "1. For outer join, the joining keys must be passed in as a Seq of strings.\n",
    "\n",
    "2. By default the joining keys from the left table are kept. \n",
    "you can still refer to keys in the right table by specifying the table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+\n",
      "|DepartmentID|  LastName|DepartmentName|\n",
      "+------------+----------+--------------+\n",
      "|          31|  Rafferty|         Sales|\n",
      "|          33|     Jones|   Engineering|\n",
      "|          33|Heisenberg|   Engineering|\n",
      "|          34|  Robinson|      Clerical|\n",
      "|          34|     Smith|      Clerical|\n",
      "|          50|       Ben|          null|\n",
      "|        null|  Williams|          null|\n",
      "+------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------+\n",
      "|DepartmentID|LastName|DepartmentName|\n",
      "+------------+--------+--------------+\n",
      "|          50|     Ben|          null|\n",
      "|        null|Williams|          null|\n",
      "+------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").\n",
    "    filter(departments(\"DepartmentID\").isNull).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Resolved attribute(s) DepartmentID#21 missing from DepartmentID#6,LastName#5,DepartmentName#22 in operator !Project [DepartmentID#6, LastName#5, DepartmentName#22, isnotnull(DepartmentID#21) AS has_depart#95]. Attribute(s) with the same name appear in the operation: DepartmentID. Please check if the right attribute(s) are used.;;\n",
       "!Project [DepartmentID#6, LastName#5, DepartmentName#22, isnotnull(DepartmentID#21) AS has_depart#95]\n",
       "+- Project [DepartmentID#6, LastName#5, DepartmentName#22]\n",
       "   +- Join LeftOuter, (DepartmentID#6 = DepartmentID#21)\n",
       "      :- Project [_1#2 AS LastName#5, _2#3 AS DepartmentID#6]\n",
       "      :  +- LocalRelation [_1#2, _2#3]\n",
       "      +- Project [_1#18 AS DepartmentID#21, _2#19 AS DepartmentName#22]\n",
       "         +- LocalRelation [_1#18, _2#19]\n",
       "\n",
       "StackTrace: !Project [DepartmentID#6, LastName#5, DepartmentName#22, isnotnull(DepartmentID#21) AS has_depart#95]\n",
       "+- Project [DepartmentID#6, LastName#5, DepartmentName#22]\n",
       "   +- Join LeftOuter, (DepartmentID#6 = DepartmentID#21)\n",
       "      :- Project [_1#2 AS LastName#5, _2#3 AS DepartmentID#6]\n",
       "      :  +- LocalRelation [_1#2, _2#3]\n",
       "      +- Project [_1#18 AS DepartmentID#21, _2#19 AS DepartmentName#22]\n",
       "         +- LocalRelation [_1#18, _2#19]\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n",
       "  at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2252)\n",
       "  at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2219)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").\n",
    "    withColumn(\"has_depart\", departments(\"DepartmentID\").isNotNull).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Resolved attribute(s) DepartmentID#21 missing from DepartmentID#6,LastName#5,DepartmentName#22 in operator !Project [isnotnull(DepartmentID#21) AS has_depart#99]. Attribute(s) with the same name appear in the operation: DepartmentID. Please check if the right attribute(s) are used.;;\n",
       "!Project [isnotnull(DepartmentID#21) AS has_depart#99]\n",
       "+- Project [DepartmentID#6, LastName#5, DepartmentName#22]\n",
       "   +- Join LeftOuter, (DepartmentID#6 = DepartmentID#21)\n",
       "      :- Project [_1#2 AS LastName#5, _2#3 AS DepartmentID#6]\n",
       "      :  +- LocalRelation [_1#2, _2#3]\n",
       "      +- Project [_1#18 AS DepartmentID#21, _2#19 AS DepartmentName#22]\n",
       "         +- LocalRelation [_1#18, _2#19]\n",
       "\n",
       "StackTrace: !Project [isnotnull(DepartmentID#21) AS has_depart#99]\n",
       "+- Project [DepartmentID#6, LastName#5, DepartmentName#22]\n",
       "   +- Join LeftOuter, (DepartmentID#6 = DepartmentID#21)\n",
       "      :- Project [_1#2 AS LastName#5, _2#3 AS DepartmentID#6]\n",
       "      :  +- LocalRelation [_1#2, _2#3]\n",
       "      +- Project [_1#18 AS DepartmentID#21, _2#19 AS DepartmentName#22]\n",
       "         +- LocalRelation [_1#18, _2#19]\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:326)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1334)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").\n",
    "    select(departments(\"DepartmentID\").isNotNull.alias(\"has_depart\")).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+----------+\n",
      "|  LastName|DepartmentID|DepartmentID|DepartmentName|has_depart|\n",
      "+----------+------------+------------+--------------+----------+\n",
      "|  Rafferty|          31|          31|         Sales|      true|\n",
      "|     Jones|          33|          33|   Engineering|      true|\n",
      "|Heisenberg|          33|          33|   Engineering|      true|\n",
      "|  Robinson|          34|          34|      Clerical|      true|\n",
      "|     Smith|          34|          34|      Clerical|      true|\n",
      "|       Ben|          50|        null|          null|     false|\n",
      "|  Williams|        null|        null|          null|     false|\n",
      "+----------+------------+------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, employees(\"DepartmentID\") === departments(\"DepartmentID\"), \"left_outer\")\n",
    "    .withColumn(\"has_depart\", departments(\"DepartmentID\").isNotNull)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|depart_id|depart_name|\n",
      "+---------+-----------+\n",
      "|       31|      Sales|\n",
      "|       33|Engineering|\n",
      "|       34|   Clerical|\n",
      "|       35|  Marketing|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "depart2 = [depart_id: int, depart_name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[depart_id: int, depart_name: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val depart2 = Seq(\n",
    "  (31, \"Sales\"), \n",
    "  (33, \"Engineering\"), \n",
    "  (34, \"Clerical\"),\n",
    "  (35, \"Marketing\")\n",
    ").toDF(\"depart_id\", \"depart_name\")\n",
    "depart2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+-----------+----------+\n",
      "|  LastName|DepartmentID|depart_id|depart_name|has_depart|\n",
      "+----------+------------+---------+-----------+----------+\n",
      "|  Rafferty|          31|       31|      Sales|      true|\n",
      "|     Jones|          33|       33|Engineering|      true|\n",
      "|Heisenberg|          33|       33|Engineering|      true|\n",
      "|  Robinson|          34|       34|   Clerical|      true|\n",
      "|     Smith|          34|       34|   Clerical|      true|\n",
      "|       Ben|          50|     null|       null|     false|\n",
      "|  Williams|        null|     null|       null|     false|\n",
      "+----------+------------+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(depart2, $\"DepartmentID\" === $\"depart_id\", \"left_outer\")\n",
    "    .withColumn(\"has_depart\", $\"depart_id\".isNotNull)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark allows using following join types: \n",
    "inner, outer, left_outer, right_outer, leftsemi. \n",
    "The interface is the same as for left outer join in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+\n",
      "|DepartmentID|  LastName|DepartmentName|\n",
      "+------------+----------+--------------+\n",
      "|          31|  Rafferty|         Sales|\n",
      "|          33|Heisenberg|   Engineering|\n",
      "|          33|     Jones|   Engineering|\n",
      "|          34|     Smith|      Clerical|\n",
      "|          34|  Robinson|      Clerical|\n",
      "|          35|      null|     Marketing|\n",
      "+------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|  LastName|DepartmentID|DepartmentID|DepartmentName|\n",
      "+----------+------------+------------+--------------+\n",
      "|  Rafferty|          31|          31|         Sales|\n",
      "|     Jones|          33|          33|   Engineering|\n",
      "|Heisenberg|          33|          33|   Engineering|\n",
      "|  Robinson|          34|          34|      Clerical|\n",
      "|     Smith|          34|          34|      Clerical|\n",
      "|       Ben|          50|        null|          null|\n",
      "|  Williams|        null|        null|          null|\n",
      "+----------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, employees(\"DepartmentID\") === departments(\"DepartmentID\"), \"left_outer\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+\n",
      "|DepartmentID|  LastName|DepartmentName|\n",
      "+------------+----------+--------------+\n",
      "|          31|  Rafferty|         Sales|\n",
      "|          33|     Jones|   Engineering|\n",
      "|          33|Heisenberg|   Engineering|\n",
      "|          34|  Robinson|      Clerical|\n",
      "|          34|     Smith|      Clerical|\n",
      "|          50|       Ben|          null|\n",
      "|        null|  Williams|          null|\n",
      "+------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+--------------+\n",
      "|LastName|DepartmentID|DepartmentID|DepartmentName|\n",
      "+--------+------------+------------+--------------+\n",
      "|     Ben|          50|        null|          null|\n",
      "|Williams|        null|        null|          null|\n",
      "+--------+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, employees(\"DepartmentID\") === departments(\"DepartmentID\"), \"left_outer\").\n",
    "    filter(departments(\"DepartmentID\").isNull).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------+\n",
      "|DepartmentID|LastName|DepartmentName|\n",
      "+------------+--------+--------------+\n",
      "|          50|     Ben|          null|\n",
      "|        null|Williams|          null|\n",
      "+------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").\n",
    "    filter(departments(\"DepartmentID\").isNull).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------+\n",
      "|DepartmentID|LastName|DepartmentName|\n",
      "+------------+--------+--------------+\n",
      "|        null|Williams|          null|\n",
      "+------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").\n",
    "    filter(employees(\"DepartmentID\").isNull).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------+\n",
      "|DepartmentID|LastName|DepartmentName|\n",
      "+------------+--------+--------------+\n",
      "|        null|Williams|          null|\n",
      "+------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, Seq(\"DepartmentID\"), \"left_outer\").\n",
    "    filter($\"DepartmentID\".isNull).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartesian Join\n",
    "\n",
    "Notice that you have to have \"spark.sql.crossJoin.enabled\" set to `true` \n",
    "in order to perform cartesian join on 2 DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sparkSession = SparkSession.\n",
    "    builder().\n",
    "    appName(\"Spark SQL basic example\").\n",
    "    config(\"spark.sql.crossJoin.enabled\", \"true\").\n",
    "    getOrCreate()\n",
    "import sparkSession.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------+\n",
      "|  LastName|DepartmentID|DepartmentID|DepartmentName|\n",
      "+----------+------------+------------+--------------+\n",
      "|  Rafferty|          31|          31|         Sales|\n",
      "|  Rafferty|          31|          33|   Engineering|\n",
      "|  Rafferty|          31|          34|      Clerical|\n",
      "|  Rafferty|          31|          35|     Marketing|\n",
      "|     Jones|          33|          31|         Sales|\n",
      "|     Jones|          33|          33|   Engineering|\n",
      "|     Jones|          33|          34|      Clerical|\n",
      "|     Jones|          33|          35|     Marketing|\n",
      "|Heisenberg|          33|          31|         Sales|\n",
      "|Heisenberg|          33|          33|   Engineering|\n",
      "|Heisenberg|          33|          34|      Clerical|\n",
      "|Heisenberg|          33|          35|     Marketing|\n",
      "|  Robinson|          34|          31|         Sales|\n",
      "|  Robinson|          34|          33|   Engineering|\n",
      "|  Robinson|          34|          34|      Clerical|\n",
      "|  Robinson|          34|          35|     Marketing|\n",
      "|     Smith|          34|          31|         Sales|\n",
      "|     Smith|          34|          33|   Engineering|\n",
      "|     Smith|          34|          34|      Clerical|\n",
      "|     Smith|          34|          35|     Marketing|\n",
      "+----------+------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+-----+\n",
      "| name| startDate|   endDate|price|\n",
      "+-----+----------+----------+-----+\n",
      "|steak|1990-01-01|2000-01-01|  150|\n",
      "|steak|2000-01-02|2020-01-01|  180|\n",
      "| fish|1990-01-01|2020-01-01|  100|\n",
      "+-----+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val products = Seq(\n",
    "  (\"steak\", \"1990-01-01\", \"2000-01-01\", 150),\n",
    "  (\"steak\", \"2000-01-02\", \"2020-01-01\", 180),\n",
    "  (\"fish\", \"1990-01-01\", \"2020-01-01\", 100)\n",
    ").toDF(\"name\", \"startDate\", \"endDate\", \"price\")\n",
    "\n",
    "products.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      date|product|\n",
      "+----------+-------+\n",
      "|1995-01-01|  steak|\n",
      "|2000-01-01|   fish|\n",
      "|2005-01-01|  steak|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val orders = Seq(\n",
    "  (\"1995-01-01\", \"steak\"),\n",
    "  (\"2000-01-01\", \"fish\"),\n",
    "  (\"2005-01-01\", \"steak\")\n",
    ").toDF(\"date\", \"product\")\n",
    "\n",
    "orders.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----------+----------+-----+\n",
      "|      date|product| name| startDate|   endDate|price|\n",
      "+----------+-------+-----+----------+----------+-----+\n",
      "|1995-01-01|  steak|steak|1990-01-01|2000-01-01|  150|\n",
      "|2000-01-01|   fish| fish|1990-01-01|2020-01-01|  100|\n",
      "|2005-01-01|  steak|steak|2000-01-02|2020-01-01|  180|\n",
      "+----------+-------+-----+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.\n",
    "    join(products, $\"product\" === $\"name\" && $\"date\" >= $\"startDate\" && $\"date\" <= $\"endDate\").\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}