{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date: 2020-09-05 13:04:03- Author: Ben Du- Date: 2020-09-01 16:31:49\n",
    "- Title: Using Bucketing in Spark\n",
    "- Slug: using-bucketing-in-spark\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, Spark, big data, bucket, partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. Bucketed column is only supported in Hive table at this time. \n",
    "\n",
    "2. Bucket for optimized filtering is available in Spark 2.4+.\n",
    "    For examples,\n",
    "    if the table `person` has a bucketed column `id` with an integer-compatible type,\n",
    "    then the following query in Spark 2.4+ will be optimized to avoid a scan of the whole table.\n",
    "    A few things to be aware here. \n",
    "    First, \n",
    "    you will still see a number of tasks close to the number of buckets in your Spark application.\n",
    "    This is becuase the optimized job will still have to check all buckets of the table \n",
    "    to see whether they are the right bucket corresponding to `id=123`.\n",
    "    (If yes, Spark will scan all rows in the bucket to filter records.\n",
    "    If not, the bucket will skipped to save time.)\n",
    "    Second, \n",
    "    the type of the value to compare must be compartible in order for Spark SQL to leverage bucket filtering.\n",
    "    For example,\n",
    "    if the `id` column in the `person` table is of the BigInt type \n",
    "    and `id = 123` is changed to `id = \"123\"` in the following query,\n",
    "    Spark will have to do a full table scan (even if it sounds extremely stupid to do so).\n",
    "\n",
    "        :::sql\n",
    "        SELECT *\n",
    "        FROM persons\n",
    "        WHERE id = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug in `DataFrame.write.partitionBy`\n",
    "\n",
    "There is currently a bug in `DataFrame.write.partitionBy(col)`.\n",
    "If there are `p` partitions in a DataFrame `df` and the columne `col` in df has `c` distinct values,\n",
    "after calling `df.write.partitionBy(year).parquet(\"/path/to/output.parquet\")`\n",
    "there will be `c` partition directories each containing (up to) `p` files.\n",
    "This means that the written table has effectively `c * p` partitions which is probably not what the user want.\n",
    "The following examples illustrate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.0.0-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Union\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+------------------+------------------+\n",
      "|year|      date|                 x|                 y|                 z|\n",
      "+----+----------+------------------+------------------+------------------+\n",
      "|2018|2018-10-17|11101.180000000006|243019.40156300002|            150.84|\n",
      "|2018|2018-11-16|32534.199999999993|      322261.41246|191.16000000000003|\n",
      "|2018|2018-11-29|39085.419999999984|454028.14863700006|            245.46|\n",
      "|2018|2018-10-18|          10295.15|     230995.140043|122.76000000000002|\n",
      "|2018|2018-11-02|          26508.74|326394.90205799986| 189.6599999999999|\n",
      "|2018|2018-12-08|           15176.5|     309785.497157|            119.64|\n",
      "|2018|2018-12-01| 38378.45999999999|     458313.700681|            231.66|\n",
      "|2018|2018-11-15| 26415.35000000001|318606.79919499985|188.45999999999995|\n",
      "|2018|2018-11-20|          30483.56|     336089.788803|144.96000000000004|\n",
      "|2019|2019-01-07|          29843.17|     375139.756514|172.62000000000003|\n",
      "|2018|2018-10-20|           9281.41|     192773.652729|143.16000000000003|\n",
      "|2018|2018-12-20| 5015.930000000001|120790.77259400001| 46.13999999999999|\n",
      "|2018|2018-11-23|          38317.15|391639.59950300003|212.22000000000003|\n",
      "|2018|2018-12-15|11833.510000000006|276072.08876499993|            141.24|\n",
      "|2018|2018-11-03| 22926.24999999999|337144.08774000005|            210.12|\n",
      "|2018|2018-10-16|10974.380000000003|236304.82008200002|159.06000000000003|\n",
      "|2018|2018-10-22|           10779.9|234750.19368899995|150.78000000000003|\n",
      "|2018|2018-12-02| 39612.31999999999| 547282.8997540001| 315.0600000000001|\n",
      "|2018|2018-12-17|11879.250000000002|293540.57401699986|            149.82|\n",
      "|2018|2018-12-06|          22865.64|350213.23658200004|183.78000000000003|\n",
      "+----+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"../../home/media/data/daily.csv\")\n",
    "df = df.select(\n",
    "    year(\"date\").alias(\"year\"),\n",
    "    \"date\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\"\n",
    ").repartition(2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"year\").parquet(\"daily.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-261c528f-ef8b-414f-86a9-c38aa5fd736a.c000.snappy.parquet\n",
      "part-00001-261c528f-ef8b-414f-86a9-c38aa5fd736a.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls daily.parquet/year=2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"daily.parquet\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"year\").saveAsTable(\"daily_hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"daily_hive\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    create table daily_hive_2\n",
    "    using parquet     \n",
    "    partitioned by (year) as\n",
    "    select * from df\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"daily_hive_2\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Optimization Leveraging Bucketed Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Using cached findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/opt/pyenv/versions/3.7.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/opt/pyenv/versions/3.7.8/bin/python -m pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.0.0-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Union\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+\n",
      "|      date|                 x|                 y|                 z|\n",
      "+----------+------------------+------------------+------------------+\n",
      "|2019-01-11|               0.0|               0.0|               0.0|\n",
      "|2019-01-10| 30436.96000000001|               0.0|               0.0|\n",
      "|2019-01-09|          30132.28|     212952.094433|            128.52|\n",
      "|2019-01-08|29883.240000000005|      352014.45016|            192.18|\n",
      "|2019-01-07|          29843.17|     375139.756514|172.62000000000003|\n",
      "|2019-01-06|          29520.23| 420714.7821390001|            217.98|\n",
      "|2019-01-05|          29308.36|376970.94769900007|             183.3|\n",
      "|2019-01-04|31114.940000000013|339321.70448899985|174.59999999999997|\n",
      "|2019-01-03|          30953.24|383834.70136999997|            197.52|\n",
      "|2019-01-02|          29647.83|     379943.385348|             199.2|\n",
      "|2019-01-01| 9098.830000000004|     221854.328826|             88.26|\n",
      "|2018-12-31|3522.9299999999994| 76976.74379300003| 30.83999999999999|\n",
      "|2018-12-30| 7722.129999999999|     210282.286054| 85.80000000000003|\n",
      "|2018-12-29|           7731.69|     184870.553121|             88.86|\n",
      "|2018-12-28| 7838.080000000003|     203588.781784|              84.6|\n",
      "|2018-12-27| 8031.669999999997|245940.99543200003|             90.84|\n",
      "|2018-12-26| 8819.809999999998|     194513.682991|             77.52|\n",
      "|2018-12-25| 6549.109999999998|136605.95935000002| 65.75999999999999|\n",
      "|2018-12-24|           5015.84| 87121.95556000003|             44.52|\n",
      "|2018-12-23| 5145.909999999998|     137563.979567|             52.02|\n",
      "+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"../../home/media/data/daily.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(10, \"date\").saveAsTable(\"daily_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"daily_b\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the execution plan does leverage bucketed columns for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [date#53, x#54, y#55, z#56]\n",
      "+- *(1) Filter (isnotnull(date#53) AND (date#53 = 2019-01-11))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet default.daily_b[date#53,x#54,y#55,z#56] Batched: true, DataFilters: [isnotnull(date#53), (date#53 = 2019-01-11)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/spark-3.0.0-bin-hadoop3.2/warehouse/daily_b], PartitionFilters: [], PushedFilters: [IsNotNull(date), EqualTo(date,2019-01-11)], ReadSchema: struct<date:string,x:string,y:string,z:string>, SelectedBucketsCount: 1 out of 10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        * \n",
    "    from \n",
    "        daily_b\n",
    "    where\n",
    "        date = \"2019-01-11\"\n",
    "    \"\"\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-2.3.4-bin-hadoop2.7/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark23 = SparkSession.builder.appName(\"PySpark_Union\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+\n",
      "|      date|                 x|                 y|                 z|\n",
      "+----------+------------------+------------------+------------------+\n",
      "|2019-01-11|               0.0|               0.0|               0.0|\n",
      "|2019-01-10| 30436.96000000001|               0.0|               0.0|\n",
      "|2019-01-09|          30132.28|     212952.094433|            128.52|\n",
      "|2019-01-08|29883.240000000005|      352014.45016|            192.18|\n",
      "|2019-01-07|          29843.17|     375139.756514|172.62000000000003|\n",
      "|2019-01-06|          29520.23| 420714.7821390001|            217.98|\n",
      "|2019-01-05|          29308.36|376970.94769900007|             183.3|\n",
      "|2019-01-04|31114.940000000013|339321.70448899985|174.59999999999997|\n",
      "|2019-01-03|          30953.24|383834.70136999997|            197.52|\n",
      "|2019-01-02|          29647.83|     379943.385348|             199.2|\n",
      "|2019-01-01| 9098.830000000004|     221854.328826|             88.26|\n",
      "|2018-12-31|3522.9299999999994| 76976.74379300003| 30.83999999999999|\n",
      "|2018-12-30| 7722.129999999999|     210282.286054| 85.80000000000003|\n",
      "|2018-12-29|           7731.69|     184870.553121|             88.86|\n",
      "|2018-12-28| 7838.080000000003|     203588.781784|              84.6|\n",
      "|2018-12-27| 8031.669999999997|245940.99543200003|             90.84|\n",
      "|2018-12-26| 8819.809999999998|     194513.682991|             77.52|\n",
      "|2018-12-25| 6549.109999999998|136605.95935000002| 65.75999999999999|\n",
      "|2018-12-24|           5015.84| 87121.95556000003|             44.52|\n",
      "|2018-12-23| 5145.909999999998|     137563.979567|             52.02|\n",
      "+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark23.read.option(\"header\", \"true\").csv(\"../../home/media/data/daily.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(10, \"date\").saveAsTable(\"daily_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark23.table(\"daily_b\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the execution plan does not leverage bucketed columns for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [date#44, x#45, y#46, z#47]\n",
      "+- *(1) Filter (isnotnull(date#44) && (date#44 = 2019-01-11))\n",
      "   +- *(1) FileScan parquet default.daily_b[date#44,x#45,y#46,z#47] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/opt/spark-2.3.4-bin-hadoop2.7/warehouse/daily_b], PartitionFilters: [], PushedFilters: [IsNotNull(date), EqualTo(date,2019-01-11)], ReadSchema: struct<date:string,x:string,y:string,z:string>\n"
     ]
    }
   ],
   "source": [
    "spark23.sql(\"\"\"\n",
    "    select \n",
    "        * \n",
    "    from \n",
    "        daily_b\n",
    "    where\n",
    "        date = \"2019-01-11\"\n",
    "    \"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
