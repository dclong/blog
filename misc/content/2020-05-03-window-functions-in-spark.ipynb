{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du- Date: 2020-05-03 23:52:16\n",
    "- Title: Window Functions in Spark\n",
    "- Slug: window-functions-in-spark\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, Spark, window function, partition, over, analytics functions, big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html\n",
    "\n",
    "https://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.sql.functions$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754039b8-8433-4353-a57a-150056645fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.11 2.4.3\n",
    "org.apache.spark spark-sql_2.11 2.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@3de37c80"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Spark Column Example\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+\n",
      "| name|      date|amount| id|\n",
      "+-----+----------+------+---+\n",
      "|Alice|2016-05-01|  50.0|  1|\n",
      "|Alice|2016-05-01|  45.0|  2|\n",
      "|Alice|2016-05-02|  55.0|  3|\n",
      "|Alice|2016-05-02| 100.0|  4|\n",
      "|  Bob|2016-05-01|  29.0|  6|\n",
      "|  Bob|2016-05-01|  25.0|  5|\n",
      "|  Bob|2016-05-02|  27.0|  7|\n",
      "|  Bob|2016-05-02|  30.0|  8|\n",
      "+-----+----------+------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customers = Seq(\n",
    "    (\"Alice\", \"2016-05-01\", 50.00, 1),\n",
    "    (\"Alice\", \"2016-05-01\", 45.00, 2),\n",
    "    (\"Alice\", \"2016-05-02\", 55.00, 3),\n",
    "    (\"Alice\", \"2016-05-02\", 100.00, 4),\n",
    "    (\"Bob\", \"2016-05-01\", 25.00, 5),\n",
    "    (\"Bob\", \"2016-05-01\", 29.00, 6),\n",
    "    (\"Bob\", \"2016-05-02\", 27.00,7 ),\n",
    "    (\"Bob\", \"2016-05-02\", 30.00, 8)\n",
    ").toDF(\"name\", \"date\", \"amount\", \"id\")\n",
    "customers.orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window with orderBy\n",
    "\n",
    "It is tricky!!!\n",
    "\n",
    "If you provide ORDER BY clause then the default frame is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW:\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/52273186/pyspark-spark-window-function-first-last-issue\n",
    "\n",
    "1. Avoid using last and use first with `descending order by` instead.\n",
    "   This gives less surprisings.\n",
    "   \n",
    "2. Do NOT use order by if not necessary. \n",
    "   It introduces unnecessary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@295c46f0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wSpec = Window.partitionBy(\"name\", \"date\").orderBy(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|     100.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      29.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      30.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    max($\"amount\").over(Window.partitionBy(\"name\", \"date\")).alias(\"max_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|     100.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      29.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      30.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        max(amount) over (partition by name, date) as max_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    max($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"max_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        max(amount) over (partition by name, date order by id) as max_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  50.0|  1|        50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|        50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|        55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|        55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        25.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        27.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    first($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"first_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  50.0|  1|        50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|        50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|        55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|        55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        25.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        27.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        first(amount) over (partition by name, date order by id) as first_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  50.0|  1|       50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|Alice|2016-05-02|  55.0|  3|       55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    last($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"last_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  50.0|  1|       50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-02|  55.0|  3|       55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        last(amount) over (partition by name, date order by id) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-01|  50.0|  1|       45.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      100.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       29.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    first($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy($\"id\".desc)).alias(\"last_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-01|  50.0|  1|       45.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      100.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       29.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        first(amount) over (partition by name, date order by id desc) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## partition by with group by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid doing so!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " expression 'customers.`amount`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;",
     "output_type": "error",
     "text": "org.apache.spark.sql.AnalysisException: expression 'customers.`amount`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\nProject [name#9, date#10, last_amount#30]\n+- Project [name#9, date#10, amount#11, id#12, last_amount#30, last_amount#30]\n   +- Window [first(amount#11, false) windowspecdefinition(name#9, date#10, id#12 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_amount#30], [name#9, date#10], [id#12 DESC NULLS LAST]\n      +- Aggregate [name#9, date#10], [name#9, date#10, amount#11, id#12]\n         +- SubqueryAlias `customers`\n            +- Project [_1#4 AS name#9, _2#5 AS date#10, _3#6 AS amount#11, _4#7 AS id#12]\n               +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\n\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:224)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:257)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n  ... 48 elided\n",
     "traceback": [
      "\u001b[1;31morg.apache.spark.sql.AnalysisException: expression 'customers.`amount`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\u001b[0;0m",
      "\u001b[1;31mProject [name#9, date#10, last_amount#30]\u001b[0;0m",
      "\u001b[1;31m+- Project [name#9, date#10, amount#11, id#12, last_amount#30, last_amount#30]\u001b[0;0m",
      "\u001b[1;31m   +- Window [first(amount#11, false) windowspecdefinition(name#9, date#10, id#12 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_amount#30], [name#9, date#10], [id#12 DESC NULLS LAST]\u001b[0;0m",
      "\u001b[1;31m      +- Aggregate [name#9, date#10], [name#9, date#10, amount#11, id#12]\u001b[0;0m",
      "\u001b[1;31m         +- SubqueryAlias `customers`\u001b[0;0m",
      "\u001b[1;31m            +- Project [_1#4 AS name#9, _2#5 AS date#10, _3#6 AS amount#11, _4#7 AS id#12]\u001b[0;0m",
      "\u001b[1;31m               +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\u001b[0;0m",
      "\u001b[1;31m\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:224)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        first(amount) over (partition by name, date order by id desc) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    group by\n",
    "        name, date\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " expression 'customers.`id`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;",
     "output_type": "error",
     "text": "org.apache.spark.sql.AnalysisException: expression 'customers.`id`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\nProject [name#9, date#10, last_amount#36]\n+- Project [name#9, date#10, _w0#39, id#12, last_amount#36, last_amount#36]\n   +- Window [first(_w0#39, false) windowspecdefinition(name#9, date#10, id#12 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_amount#36], [name#9, date#10], [id#12 DESC NULLS LAST]\n      +- Aggregate [name#9, date#10], [name#9, date#10, max(amount#11) AS _w0#39, id#12]\n         +- SubqueryAlias `customers`\n            +- Project [_1#4 AS name#9, _2#5 AS date#10, _3#6 AS amount#11, _4#7 AS id#12]\n               +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\n\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:224)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:257)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n  ... 48 elided\n",
     "traceback": [
      "\u001b[1;31morg.apache.spark.sql.AnalysisException: expression 'customers.`id`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\u001b[0;0m",
      "\u001b[1;31mProject [name#9, date#10, last_amount#36]\u001b[0;0m",
      "\u001b[1;31m+- Project [name#9, date#10, _w0#39, id#12, last_amount#36, last_amount#36]\u001b[0;0m",
      "\u001b[1;31m   +- Window [first(_w0#39, false) windowspecdefinition(name#9, date#10, id#12 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_amount#36], [name#9, date#10], [id#12 DESC NULLS LAST]\u001b[0;0m",
      "\u001b[1;31m      +- Aggregate [name#9, date#10], [name#9, date#10, max(amount#11) AS _w0#39, id#12]\u001b[0;0m",
      "\u001b[1;31m         +- SubqueryAlias `customers`\u001b[0;0m",
      "\u001b[1;31m            +- Project [_1#4 AS name#9, _2#5 AS date#10, _3#6 AS amount#11, _4#7 AS id#12]\u001b[0;0m",
      "\u001b[1;31m               +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\u001b[0;0m",
      "\u001b[1;31m\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:224)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        first(max(amount)) over (partition by name, date order by id desc) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    group by\n",
    "        name, date\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+\n",
      "| name|      date|amount| id|\n",
      "+-----+----------+------+---+\n",
      "|Alice|2016-05-01|  50.0|  1|\n",
      "|Alice|2016-05-01|  45.0|  2|\n",
      "|Alice|2016-05-02|  55.0|  3|\n",
      "|Alice|2016-05-02| 100.0|  4|\n",
      "|  Bob|2016-05-01|  29.0|  6|\n",
      "|  Bob|2016-05-01|  25.0|  5|\n",
      "|  Bob|2016-05-02|  27.0|  7|\n",
      "|  Bob|2016-05-02|  30.0|  8|\n",
      "+-----+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+------+\n",
      "| name|      date|amount| id|rownum|\n",
      "+-----+----------+------+---+------+\n",
      "|Alice|2016-05-01|  45.0|  2|     1|\n",
      "|Alice|2016-05-01|  50.0|  1|     2|\n",
      "|Alice|2016-05-02| 100.0|  4|     1|\n",
      "|Alice|2016-05-02|  55.0|  3|     2|\n",
      "|  Bob|2016-05-01|  29.0|  6|     1|\n",
      "|  Bob|2016-05-01|  25.0|  5|     2|\n",
      "|  Bob|2016-05-02|  30.0|  8|     1|\n",
      "|  Bob|2016-05-02|  27.0|  7|     2|\n",
      "+-----+----------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        row_number() over (partition by name, date order by id desc) as rownum\n",
    "    from\n",
    "        customers\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---+------+\n",
      "| name|      date|amount| id|rownum|\n",
      "+-----+----------+------+---+------+\n",
      "|Alice|2016-05-01|  45.0|  2|     1|\n",
      "|Alice|2016-05-02| 100.0|  4|     1|\n",
      "|  Bob|2016-05-01|  29.0|  6|     1|\n",
      "|  Bob|2016-05-02|  30.0|  8|     1|\n",
      "+-----+----------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        *\n",
    "    from (\n",
    "        select\n",
    "            name,\n",
    "            date,\n",
    "            amount,\n",
    "            id,\n",
    "            row_number() over (partition by name, date order by id desc) as rownum\n",
    "        from\n",
    "            customers\n",
    "        ) A\n",
    "    where \n",
    "        rownum = 1\n",
    "    \"\"\").orderBy(\"name\", \"date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
