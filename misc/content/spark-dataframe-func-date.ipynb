{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-09-05 18:19:33\n",
    "- Title: Date Functions in Spark\n",
    "- Slug: spark-dataframe-func-date\n",
    "- Category: Computer Science\n",
    "- Tags: programming, PySpark, Spark, DataFrame, date, Spark SQL, functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "It seems that these functions work on string of the format \"YYYY-mm-dd\".\n",
    "Check whether they work on other formats!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "findspark.init(str(next(Path(\"/opt\").glob(\"spark-3*\"))))\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Str_Func\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +/- Operators\n",
    "\n",
    "The +/- operators are supported on date/time columns in Spark 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2020-09-05|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date as today\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2020-09-15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date + 10 as today\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2020-09-04|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date - 1 as today\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        d1|        d2|\n",
      "+----------+----------+\n",
      "|2017-01-01|2017-01-07|\n",
      "|2017-02-01|2019-02-10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (\"2017-01-01\", \"2017-01-07\"),\n",
    "    (\"2017-02-01\", \"2019-02-10\")\n",
    ").toDF(\"d1\", \"d2\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+-----+\n",
      "|        d1|        d2|        d3|        d4|check|\n",
      "+----------+----------+----------+----------+-----+\n",
      "|2017-01-01|2017-01-07|2016-12-02|2017-01-31| true|\n",
      "|2017-02-01|2019-02-10|2017-01-02|2017-03-03|false|\n",
      "+----------+----------+----------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(d3,DateType,true), StructField(d4,DateType,true), StructField(check,BooleanType,true)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df1 = df.withColumn(\"d3\", date_sub($\"d1\", 30))\n",
    "    .withColumn(\"d4\", date_add($\"d1\", 30))\n",
    "    .withColumn(\"check\", $\"d2\".between($\"d3\", $\"d4\"))\n",
    "df1.show\n",
    "df1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|        d1|        d2|diff|\n",
      "+----------+----------+----+\n",
      "|2017-01-01|2017-01-07|   6|\n",
      "|2017-02-01|2019-02-10| 739|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(diff,IntegerType,true)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.withColumn(\"diff\", datediff($\"d2\", $\"d1\"))\n",
    "df2.show\n",
    "df2.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2|   current|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|2018-05-02|\n",
      "|2017-02-01|2019-02-10|2018-05-02|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(current,DateType,false)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = df.withColumn(\"current\", current_date())\n",
    "df3.show\n",
    "df3.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|        d1|        d2|day_of_d2|\n",
      "+----------+----------+---------+\n",
      "|2017-01-01|2017-01-07|        7|\n",
      "|2017-02-01|2019-02-10|       10|\n",
      "+----------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(day_of_d2,IntegerType,true)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4 = df.withColumn(\"day_of_d2\", dayofmonth($\"d2\"))\n",
    "df4.show\n",
    "df4.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+--------------+\n",
      "|        d1|        d2|day_of_year_d1|day_of_year_d2|\n",
      "+----------+----------+--------------+--------------+\n",
      "|2017-01-01|2017-01-07|             1|             7|\n",
      "|2017-02-01|2019-02-10|            32|            41|\n",
      "+----------+----------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(day_of_year_d1,IntegerType,true), StructField(day_of_year_d2,IntegerType,true)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df5 = df.withColumn(\"day_of_year_d1\", dayofyear($\"d1\")).withColumn(\"day_of_year_d2\", dayofyear($\"d2\"))\n",
    "df5.show\n",
    "df5.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2| format_d1|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|01/01/2017|\n",
      "|2017-02-01|2019-02-10|01/02/2017|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(format_d1,StringType,true)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df6 = df.withColumn(\"format_d1\", date_format($\"d1\", \"dd/MM/yyyy\"))\n",
    "df6.show\n",
    "df6.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        month(\"2018-01-01\") as month\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2018|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        year(\"2018-01-01\") as year\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}