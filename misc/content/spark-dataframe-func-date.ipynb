{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-09-05 18:19:33\n",
    "- Title: Date Functions in Spark\n",
    "- Slug: spark-dataframe-func-date\n",
    "- Category: Computer Science\n",
    "- Tags: programming, PySpark, Spark, DataFrame, date, Spark SQL, function, SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. Most date functions work on a string of the format `yyyy-MM-dd`\n",
    "    which is automatically casted to a date object.\n",
    "    \n",
    "2. Functions `second`, `minute`, `day`/`dayofmonth`, `weekofyear`, `monthofyear`, `quarter` \n",
    "    and `year` extract the corresponding part from a date object/string.\n",
    "    \n",
    "3. `date_add`, `date_sub`, `datediff` and `add_months` performs arithmatical operations on dates.\n",
    "\n",
    "4. `to_date`, `to_timestamp`, `to_utc_timestamp`, `to_unix_timestamp` and `timestamp` \n",
    "    cast date objects/strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "findspark.init(str(next(Path(\"/opt\").glob(\"spark-3*\"))))\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Str_Func\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +/- Operators\n",
    "\n",
    "The +/- operators are supported on date/time columns in Spark 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2020-09-05|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date as today\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2020-09-15|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date + 10 as today\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2020-09-04|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date - 1 as today\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add_months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        d1|        d2|\n",
      "+----------+----------+\n",
      "|2017-01-01|2017-01-07|\n",
      "|2017-02-01|2019-02-10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (\"2017-01-01\", \"2017-01-07\"),\n",
    "    (\"2017-02-01\", \"2019-02-10\")\n",
    ").toDF(\"d1\", \"d2\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+-----+\n",
      "|        d1|        d2|        d3|        d4|check|\n",
      "+----------+----------+----------+----------+-----+\n",
      "|2017-01-01|2017-01-07|2016-12-02|2017-01-31| true|\n",
      "|2017-02-01|2019-02-10|2017-01-02|2017-03-03|false|\n",
      "+----------+----------+----------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(d3,DateType,true), StructField(d4,DateType,true), StructField(check,BooleanType,true)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df1 = df.withColumn(\"d3\", date_sub($\"d1\", 30))\n",
    "    .withColumn(\"d4\", date_add($\"d1\", 30))\n",
    "    .withColumn(\"check\", $\"d2\".between($\"d3\", $\"d4\"))\n",
    "df1.show\n",
    "df1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_trunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|        d1|        d2|diff|\n",
      "+----------+----------+----+\n",
      "|2017-01-01|2017-01-07|   6|\n",
      "|2017-02-01|2019-02-10| 739|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(diff,IntegerType,true)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.withColumn(\"diff\", datediff($\"d2\", $\"d1\"))\n",
    "df2.show\n",
    "df2.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2|   current|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|2018-05-02|\n",
      "|2017-02-01|2019-02-10|2018-05-02|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(current,DateType,false)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = df.withColumn(\"current\", current_date())\n",
    "df3.show\n",
    "df3.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_timestamp / now\n",
    "\n",
    "Both `current_timestamp` and `now` returns the current timestamp.\n",
    "The difference is that `now` must be called with parentheses \n",
    "while `curent_timestamp` can be called without parentheses.\n",
    "If you are not sure, \n",
    "always call functions with parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp()    |\n",
      "+-----------------------+\n",
      "|2020-09-07 10:58:58.381|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        current_timestamp\n",
    "    \"\"\").show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|current_timestamp()   |\n",
      "+----------------------+\n",
      "|2020-09-07 11:00:53.47|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        current_timestamp()\n",
    "    \"\"\").show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|now()                  |\n",
      "+-----------------------+\n",
      "|2020-09-07 10:59:37.629|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        now()\n",
    "    \"\"\").show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dayofmonth / day\n",
    "\n",
    "Returns the day from a given date or timestamp. This function is the same as the day function.\n",
    "`dayofmonth` is the same as the `day` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|        d1|        d2|day_of_d2|\n",
      "+----------+----------+---------+\n",
      "|2017-01-01|2017-01-07|        7|\n",
      "|2017-02-01|2019-02-10|       10|\n",
      "+----------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(day_of_d2,IntegerType,true)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4 = df.withColumn(\"day_of_d2\", dayofmonth($\"d2\"))\n",
    "df4.show\n",
    "df4.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|dayofmonth(CAST(2017-01-07 AS DATE))|\n",
      "+------------------------------------+\n",
      "|                                   7|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        dayofmonth(\"2017-01-07\") \n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+--------------+\n",
      "|        d1|        d2|day_of_year_d1|day_of_year_d2|\n",
      "+----------+----------+--------------+--------------+\n",
      "|2017-01-01|2017-01-07|             1|             7|\n",
      "|2017-02-01|2019-02-10|            32|            41|\n",
      "+----------+----------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(day_of_year_d1,IntegerType,true), StructField(day_of_year_d2,IntegerType,true)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df5 = df.withColumn(\"day_of_year_d1\", dayofyear($\"d1\")).withColumn(\"day_of_year_d2\", dayofyear($\"d2\"))\n",
    "df5.show\n",
    "df5.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2| format_d1|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|01/01/2017|\n",
      "|2017-02-01|2019-02-10|01/02/2017|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(format_d1,StringType,true)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df6 = df.withColumn(\"format_d1\", date_format($\"d1\", \"dd/MM/yyyy\"))\n",
    "df6.show\n",
    "df6.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date_trunc\n",
    "\n",
    "Returns a timestamp specified as (ts) truncated to the unit specified by format (fmt) \n",
    "[“YEAR”, “YYYY”, “YY”, “MON”, “MONTH”, “MM”, “DAY”, “DD”, “HOUR”, “MINUTE”, “SECOND”, “WEEK”, “QUARTER”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        month(\"2018-01-01\") as month\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now\n",
    "\n",
    "Returns the current timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next_day\n",
    "\n",
    "Returns the day after the start_date specified by day_of_week. \n",
    "Day of week can be specified as ‘MON’, ‘TUE’, ‘WED’, ‘THU’, ‘FRI’, ‘SAT’, ‘SUN’ \n",
    "or as ‘MO’, ‘TU’, ‘WE’, ‘TH’, ‘FR’, ‘SA’, ‘SU’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_utc_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weekofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2018|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        year(\"2018-01-01\") as year\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://obstkel.com/spark-sql-functions\n",
    "\n",
    "https://obstkel.com/spark-sql-date-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
