{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Conversion Between PySpark DataFrames and pandas DataFrames\n",
    "- Slug: pyspark-pandas-dataframe-conversion\n",
    "- Date: 2020-06-18 08:43:44\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Python, HPC, high performance computing, PySpark, DataFrame, construct\n",
    "- Author: Ben Du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "1. A PySpark DataFrame can be converted to a pandas DataFrame by calling the method `DataFrame.toPandas`,\n",
    "    and a pandas DataFrame can be converted to a PySpark DataFrame by calling `SparkSession.createDataFrame`.\n",
    "    Notice that when you call `DataFrame.toPandas` \n",
    "    to convert a Spark DataFrame to a pandas DataFrame, \n",
    "    the whole Spark DataFrame is collected to the driver machine!\n",
    "    This means that you should only call the method `DataFrame.toPandas`\n",
    "    when the Spark DataFrame is small enough to fit into the memory of the driver machine.\n",
    "\n",
    "2. Apache Arrow can be leveraged to convert between Spark DataFrame and pandas DataFrame without data copying. \n",
    "    However, \n",
    "    there are some restrictions on this.\n",
    "    Please refer to \n",
    "    [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html)\n",
    "    for more discussions.\n",
    "\n",
    "2. The perhaps most convenient way to create an ad hoc PySpark DataFrame \n",
    "    is to first [create a pandas DataFrame](http://www.legendu.net/en/blog/construct-pandas-dataframe-python/)\n",
    "    and then convert it to a PySpark DataFrame (using `SparkSession.createDataFrame`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_pandas\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dan</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  id  age\n",
       "0   Ben   2   30\n",
       "1   Dan   4   25\n",
       "2  Will   1   26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p = pd.DataFrame(data=[\n",
    "    [\"Ben\", 2, 30],\n",
    "    [\"Dan\", 4, 25],\n",
    "    [\"Will\", 1, 26],\n",
    "], columns=[\"name\", \"id\", \"age\"])\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|name| id|age|\n",
      "+----+---+---+\n",
      "| Ben|  2| 30|\n",
      "| Dan|  4| 25|\n",
      "|Will|  1| 26|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df_p)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://stackoverflow.com/questions/37612622/spark-unionall-multiple-dataframes\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}