{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Rename and Drop Columns in Spark DataFrames\n",
    "- Slug: spark-rename-columns\n",
    "- Date: 2020-07-19 14:24:40\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, rename, column, drop\n",
    "- Author: Ben Du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "You can use `withColumnRenamed` to rename a column in a DataFrame.\n",
    "You can also do renaming using `alias` when select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.load.ivy(\"org.apache.spark\" % \"spark-core_2.12\" % \"3.0.0\")\n",
    "interp.load.ivy(\"org.apache.spark\" % \"spark-sql_2.12\" % \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/07/19 14:26:45 INFO SparkContext: Running Spark version 3.0.0\n",
      "20/07/19 14:26:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/07/19 14:26:46 INFO ResourceUtils: ==============================================================\n",
      "20/07/19 14:26:46 INFO ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "20/07/19 14:26:46 INFO ResourceUtils: ==============================================================\n",
      "20/07/19 14:26:46 INFO SparkContext: Submitted application: Spark_DataFrame_Column\n",
      "20/07/19 14:26:46 INFO SecurityManager: Changing view acls to: gitpod\n",
      "20/07/19 14:26:46 INFO SecurityManager: Changing modify acls to: gitpod\n",
      "20/07/19 14:26:46 INFO SecurityManager: Changing view acls groups to: \n",
      "20/07/19 14:26:46 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/07/19 14:26:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gitpod); groups with view permissions: Set(); users  with modify permissions: Set(gitpod); groups with modify permissions: Set()\n",
      "20/07/19 14:26:47 INFO Utils: Successfully started service 'sparkDriver' on port 33127.\n",
      "20/07/19 14:26:47 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/07/19 14:26:47 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/07/19 14:26:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/07/19 14:26:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/07/19 14:26:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "20/07/19 14:26:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-656b8348-9d21-4a40-9379-87472584ffb8\n",
      "20/07/19 14:26:47 INFO MemoryStore: MemoryStore started with capacity 1172.4 MiB\n",
      "20/07/19 14:26:47 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/07/19 14:26:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/07/19 14:26:47 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "20/07/19 14:26:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ws-2d470c6b-0983-4974-8000-c7b96edf3082:4041\n",
      "20/07/19 14:26:48 INFO Executor: Starting executor ID driver on host ws-2d470c6b-0983-4974-8000-c7b96edf3082\n",
      "20/07/19 14:26:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45133.\n",
      "20/07/19 14:26:48 INFO NettyBlockTransferService: Server created on ws-2d470c6b-0983-4974-8000-c7b96edf3082:45133\n",
      "20/07/19 14:26:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/07/19 14:26:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ws-2d470c6b-0983-4974-8000-c7b96edf3082, 45133, None)\n",
      "20/07/19 14:26:48 INFO BlockManagerMasterEndpoint: Registering block manager ws-2d470c6b-0983-4974-8000-c7b96edf3082:45133 with 1172.4 MiB RAM, BlockManagerId(driver, ws-2d470c6b-0983-4974-8000-c7b96edf3082, 45133, None)\n",
      "20/07/19 14:26:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ws-2d470c6b-0983-4974-8000-c7b96edf3082, 45133, None)\n",
      "20/07/19 14:26:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ws-2d470c6b-0983-4974-8000-c7b96edf3082, 45133, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@234915a7\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Spark_DataFrame_Column\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/07/19 14:26:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/workspace/blog/misc/content/spark-warehouse').\n",
      "20/07/19 14:26:52 INFO SharedState: Warehouse path is 'file:/workspace/blog/misc/content/spark-warehouse'.\n",
      "20/07/19 14:26:54 INFO CodeGenerator: Code generated in 517.780589 ms\n",
      "20/07/19 14:26:56 INFO CodeGenerator: Code generated in 36.279689 ms\n",
      "20/07/19 14:26:56 INFO CodeGenerator: Code generated in 30.128604 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| _1| _2| _3| _4|\n",
      "+---+---+---+---+\n",
      "|  1|  a|foo|3.0|\n",
      "|  2|  b|bar|4.0|\n",
      "|  3|  c|foo|5.0|\n",
      "|  4|  d|bar|7.0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [_1: bigint, _2: string ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (1L, \"a\", \"foo\", 3.0),\n",
    "    (2L, \"b\", \"bar\", 4.0),\n",
    "    (3L, \"c\", \"foo\", 5.0),\n",
    "    (4L, \"d\", \"bar\", 7.0)\n",
    ").toDF\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/07/19 14:27:45 INFO CodeGenerator: Code generated in 22.343436 ms\n",
      "20/07/19 14:27:45 INFO CodeGenerator: Code generated in 25.541113 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _2| _4|\n",
      "+---+---+\n",
      "|  a|3.0|\n",
      "|  b|4.0|\n",
      "|  c|5.0|\n",
      "|  d|7.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"_1\", \"_3\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming One Column Using `withColumnRenamed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| x1| _2| _3| _4|\n",
      "+---+---+---+---+\n",
      "|  1|  a|foo|3.0|\n",
      "|  2|  b|bar|4.0|\n",
      "|  3|  c|foo|5.0|\n",
      "|  4|  d|bar|7.0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"_1\", \"x1\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming One Column Using `alias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| x1| _2| _3| _4|\n",
      "+---+---+---+---+\n",
      "|  1|  a|foo|3.0|\n",
      "|  2|  b|bar|4.0|\n",
      "|  3|  c|foo|5.0|\n",
      "|  4|  d|bar|7.0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    $\"_1\".alias(\"x1\"),\n",
    "    $\"_2\",\n",
    "    $\"_3\",\n",
    "    $\"_4\"\n",
    ").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Renaming Using `withColumnRenamed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc392e3d-37d5-413b-8d5a-6754283e84da",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val lookup = Map(\n",
    "    \"_1\" -> \"x1\",\n",
    "    \"_2\" -> \"x2\",\n",
    "    \"_3\" -> \"x3\",\n",
    "    \"_4\" -> \"x4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| x1| x2| x3| x4|\n",
      "+---+---+---+---+\n",
      "|  1|  a|foo|3.0|\n",
      "|  2|  b|bar|4.0|\n",
      "|  3|  c|foo|5.0|\n",
      "|  4|  d|bar|7.0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup.foldLeft(df) {\n",
    "    (acc, ca) => acc.withColumnRenamed(ca._1, ca._2)\n",
    "}.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Renaming Using `alias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| x1| x2| x3| x4|\n",
      "+---+---+---+---+\n",
      "|  1|  a|foo|3.0|\n",
      "|  2|  b|bar|4.0|\n",
      "|  3|  c|foo|5.0|\n",
      "|  4|  d|bar|7.0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns.map(c => col(c).alias(lookup.getOrElse(c, c))): _*).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
