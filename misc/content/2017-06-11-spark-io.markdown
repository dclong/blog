UUID: 73d3a35c-1de6-4ee0-905e-36abf03bf09e
Status: published
Date: 2017-06-11 19:16:05
Author: Ben Chuanlong Du
Slug: spark-io
Title: Spark IO
Category: Programming
Tags: programming, Spark, IO, compression

**
Things on this page are
fragmentary and immature notes/thoughts of the author.
It is not meant to readers
but rather for convenient reference of the author and future improvement.
**

1. When you want to keep headers in the data, 
    it is suggested that you use the Parquet format to save data for multiple reasons.
    It is fast, takes less space
    and you don't have to worry about duplicated headers encountered when merging CSV files with headers.
    There is no need to merge Parquet files. 
    You can directly read in a folder of Parquet files in all kinds of programming languages,
    which is more convenient than a folder of CSV files.

2. Writing to existing files using `RDD.saveAsTextFile` throws file already exist exception. 
    This s because Hadoop filesystem does not overwrite files that already exist.
    `RDD.saveAsTextFile` does not provide an option to manually overwrite existing files.
    To avoid the issue, 
    you have to manually remove the existing file before writing to them.
    Of course, 
    it is no longer suggested to use RDD directly any more in Spark. 
    You use should DataFrame as much as possible. 
    `DataFrame.write` allows you to overwrite existing HDFS files via `DataFrame.write.option("overwrite")`.

## File Format

avro offical support instead of databricks? simplest way to read avro into data frame?

– Text – Sequence – Avro – Parquet – ORC

Example orc file for me to read?

spark: it seems that we must know the schema of sequence files ...


## RDD.saveAsTextFile

1. `RDD.saveAsTextFile` does not provide options to control the format of output files.
    You have to manually format the RDD to one containings string in the format that you want. 
    For example, 
    if rdd contains tuples and you want to output it into TSV files, 
    you can format it first using the following code.

        rdd.map { x => x.productIterator.mkString("\t") }

2. It takes lots of for `hadoop fs -getmerge`
    to extract and merge large number of compressed files.
    So it is suggested that you turn off compression 
    when saving results into text files (using `saveAsTextFile`)
    especially when there are huge number of partitions.

```scala
sc.hadoopConfiguration.set("mapred.output.compress", "false")
```
