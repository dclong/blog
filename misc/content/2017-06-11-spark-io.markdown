Status: published
Date: 2019-07-08 23:28:03
Author: Ben Chuanlong Du
Slug: spark-io
Title: Spark IO
Category: Programming
Tags: programming, Spark, IO, compression

**
Things on this page are
fragmentary and immature notes/thoughts of the author.
It is not meant to readers
but rather for convenient reference of the author and future improvement.
**

1. Do NOT read data from and write data to the same path in Spark!
  Due to lazy evaluation of Spark, 
  the path will likely be cleared before it's read into Spark,
  which will throw IO exceptions.
  And the worst part is that your data on HDFS is removed but recoverable.

2. When you want to keep headers in the data,
    it is suggested that you use the Parquet format to save data for multiple reasons.
    It is fast, takes less space
    and you don't have to worry about duplicated headers encountered when merging CSV files with headers.
    There is no need to merge Parquet files.
    You can directly read in a folder of Parquet files in all kinds of programming languages,
    which is more convenient than a folder of CSV files.

3. Writing to existing files using `RDD.saveAsTextFile` throws file already exist exception.
    This s because Hadoop filesystem does not overwrite files that already exist.
    `RDD.saveAsTextFile` does not provide an option to manually overwrite existing files.
    To avoid the issue,
    you have to manually remove the existing file before writing to them.
    Of course,
    it is no longer suggested to use RDD directly any more in Spark.
    You use should DataFrame as much as possible.
    `DataFrame.write` allows you to overwrite existing HDFS files via `DataFrame.write.option("overwrite")`.

## Readiness of Data on HDFS

1. Data in a Hive table guarantees completeness
    which means that if you see data of a certain date in the table,
    the complete data is there.
    However, if you work with other format (Parquet, Avro, Sequence, etc.),
    you'd better check for data readiness before you use it.
    A simple way to do this is to check whether the `_SUCESS` file exists in the same directory.


## Parquet

https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery

1. You have issues saving a DataFrame read in from Parquet to a CSV file.
    The reason is that Parquet support more complex data structures (e.g., array)
    which is not supported in CSV.

2. Shell-like syntax (curly brace, wildcard, etc.) of matching multiple files is supported in both the Scala API
    and the SQL API when querying files directly
    as long as it does cause CONFLICTING directory structure.


        files = 's3a://dev/2017/01/{02,03}/data.parquet'
        df = session.read.parquet(files)

        spark.read.parquet("some_path/2019-02-10_05-38-11/SITE_NAME=*")

        select
            count(*) as n
        from
            parquet.`/some_path/2019-02-10_05-38-11/SITE_NAME=*`

https://spark.apache.org/docs/latest/sql-data-sources-parquet.html

https://docs.databricks.com/spark/latest/data-sources/read-parquet.html

## File Format

avro offical support instead of databricks? simplest way to read avro into data frame?

– Text – Sequence – Avro – Parquet – ORC

Example orc file for me to read?

spark: it seems that we must know the schema of sequence files ...


## RDD.saveAsTextFile

1. `RDD.saveAsTextFile` does not provide options to control the format of output files.
    You have to manually format the RDD to one containings string in the format that you want.
    For example,
    if rdd contains tuples and you want to output it into TSV files,
    you can format it first using the following code.

        rdd.map { x => x.productIterator.mkString("\t") }

2. It takes lots of for `hadoop fs -getmerge`
    to extract and merge large number of compressed files.
    So it is suggested that you turn off compression
    when saving results into text files (using `saveAsTextFile`)
    especially when there are huge number of partitions.

```scala
sc.hadoopConfiguration.set("mapred.output.compress", "false")
```
