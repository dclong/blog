{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Column Functions and Operators in Spark\n",
    "- Slug: pyspark-func-operators\n",
    "- Date: 2021-01-27 00:16:21\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, column, functions, operators, func, fun\n",
    "- Author: Ben Du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "#findspark.init(str(next(Path(\"/opt\").glob(\"spark-3*\"))))\n",
    "findspark.init(\"/opt/spark-3.0.1-bin-hadoop3.2/\")\n",
    "#findspark.init(\"/opt/spark-2.3.0-bin-hadoop2.7\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, ArrayType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark_Str_Func\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+\n",
      "|  col1|col2|col3|\n",
      "+------+----+----+\n",
      "|[1, 2]| how|   1|\n",
      "|[2, 3]| are|   2|\n",
      "|[3, 4]| you|   3|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=[([1, 2], \"how\", 1), ([2, 3], \"are\", 2), ([3, 4], \"you\", 3)],\n",
    "        columns=[\"col1\", \"col2\", \"col3\"]\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Column<b'multialias(col1)'>\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df.col1.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py4j.java_gateway.JavaMember at 0x7f21084f4190>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(df.col1._jc, \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'col1'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.col1._jc.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$amp$amp',\n",
       " '$anonfun$$eq$eq$eq$1',\n",
       " '$anonfun$$less$eq$greater$1',\n",
       " '$anonfun$isin$1',\n",
       " '$anonfun$named$1',\n",
       " '$anonfun$named$2',\n",
       " '$bang$eq$eq',\n",
       " '$bar$bar',\n",
       " '$div',\n",
       " '$eq$bang$eq',\n",
       " '$eq$eq$eq',\n",
       " '$greater',\n",
       " '$greater$eq',\n",
       " '$less',\n",
       " '$less$eq',\n",
       " '$less$eq$greater',\n",
       " '$minus',\n",
       " '$percent',\n",
       " '$plus',\n",
       " '$times',\n",
       " 'alias',\n",
       " 'and',\n",
       " 'apply',\n",
       " 'as',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'between',\n",
       " 'bitwiseAND',\n",
       " 'bitwiseOR',\n",
       " 'bitwiseXOR',\n",
       " 'cast',\n",
       " 'contains',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'divide',\n",
       " 'endsWith',\n",
       " 'eqNullSafe',\n",
       " 'equalTo',\n",
       " 'equals',\n",
       " 'explain',\n",
       " 'expr',\n",
       " 'geq',\n",
       " 'getClass',\n",
       " 'getField',\n",
       " 'getItem',\n",
       " 'gt',\n",
       " 'hashCode',\n",
       " 'initializeForcefully',\n",
       " 'initializeLogIfNecessary',\n",
       " 'initializeLogIfNecessary$default$2',\n",
       " 'isInCollection',\n",
       " 'isNaN',\n",
       " 'isNotNull',\n",
       " 'isNull',\n",
       " 'isTraceEnabled',\n",
       " 'isin',\n",
       " 'leq',\n",
       " 'like',\n",
       " 'log',\n",
       " 'logDebug',\n",
       " 'logError',\n",
       " 'logInfo',\n",
       " 'logName',\n",
       " 'logTrace',\n",
       " 'logWarning',\n",
       " 'lt',\n",
       " 'minus',\n",
       " 'mod',\n",
       " 'multiply',\n",
       " 'name',\n",
       " 'named',\n",
       " 'notEqual',\n",
       " 'notify',\n",
       " 'notifyAll',\n",
       " 'or',\n",
       " 'org$apache$spark$internal$Logging$$log_',\n",
       " 'org$apache$spark$internal$Logging$$log__$eq',\n",
       " 'otherwise',\n",
       " 'over',\n",
       " 'plus',\n",
       " 'rlike',\n",
       " 'startsWith',\n",
       " 'substr',\n",
       " 'toString',\n",
       " 'unapply',\n",
       " 'unary_$bang',\n",
       " 'unary_$minus',\n",
       " 'wait',\n",
       " 'when']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(df.col1._jc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<py4j.java_gateway.JavaMember at 0x7f21088ae730>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(df.col1._jc, \"as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$amp$amp',\n",
       " '$anonfun$$eq$eq$eq$1',\n",
       " '$anonfun$$less$eq$greater$1',\n",
       " '$anonfun$isin$1',\n",
       " '$anonfun$named$1',\n",
       " '$anonfun$named$2',\n",
       " '$bang$eq$eq',\n",
       " '$bar$bar',\n",
       " '$div',\n",
       " '$eq$bang$eq',\n",
       " '$eq$eq$eq',\n",
       " '$greater',\n",
       " '$greater$eq',\n",
       " '$less',\n",
       " '$less$eq',\n",
       " '$less$eq$greater',\n",
       " '$minus',\n",
       " '$percent',\n",
       " '$plus',\n",
       " '$times',\n",
       " 'alias',\n",
       " 'and',\n",
       " 'apply',\n",
       " 'as',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'between',\n",
       " 'bitwiseAND',\n",
       " 'bitwiseOR',\n",
       " 'bitwiseXOR',\n",
       " 'cast',\n",
       " 'contains',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'divide',\n",
       " 'endsWith',\n",
       " 'eqNullSafe',\n",
       " 'equalTo',\n",
       " 'equals',\n",
       " 'explain',\n",
       " 'expr',\n",
       " 'geq',\n",
       " 'getClass',\n",
       " 'getField',\n",
       " 'getItem',\n",
       " 'gt',\n",
       " 'hashCode',\n",
       " 'initializeForcefully',\n",
       " 'initializeLogIfNecessary',\n",
       " 'initializeLogIfNecessary$default$2',\n",
       " 'isInCollection',\n",
       " 'isNaN',\n",
       " 'isNotNull',\n",
       " 'isNull',\n",
       " 'isTraceEnabled',\n",
       " 'isin',\n",
       " 'leq',\n",
       " 'like',\n",
       " 'log',\n",
       " 'logDebug',\n",
       " 'logError',\n",
       " 'logInfo',\n",
       " 'logName',\n",
       " 'logTrace',\n",
       " 'logWarning',\n",
       " 'lt',\n",
       " 'minus',\n",
       " 'mod',\n",
       " 'multiply',\n",
       " 'name',\n",
       " 'named',\n",
       " 'notEqual',\n",
       " 'notify',\n",
       " 'notifyAll',\n",
       " 'or',\n",
       " 'org$apache$spark$internal$Logging$$log_',\n",
       " 'org$apache$spark$internal$Logging$$log__$eq',\n",
       " 'otherwise',\n",
       " 'over',\n",
       " 'plus',\n",
       " 'rlike',\n",
       " 'startsWith',\n",
       " 'substr',\n",
       " 'toString',\n",
       " 'unapply',\n",
       " 'unary_$bang',\n",
       " 'unary_$minus',\n",
       " 'wait',\n",
       " 'when']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(df.col1._jc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__and__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '_asc_doc',\n",
       " '_asc_nulls_first_doc',\n",
       " '_asc_nulls_last_doc',\n",
       " '_bitwiseAND_doc',\n",
       " '_bitwiseOR_doc',\n",
       " '_bitwiseXOR_doc',\n",
       " '_contains_doc',\n",
       " '_desc_doc',\n",
       " '_desc_nulls_first_doc',\n",
       " '_desc_nulls_last_doc',\n",
       " '_endswith_doc',\n",
       " '_eqNullSafe_doc',\n",
       " '_isNotNull_doc',\n",
       " '_isNull_doc',\n",
       " '_jc',\n",
       " '_like_doc',\n",
       " '_rlike_doc',\n",
       " '_startswith_doc',\n",
       " 'alias',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'astype',\n",
       " 'between',\n",
       " 'bitwiseAND',\n",
       " 'bitwiseOR',\n",
       " 'bitwiseXOR',\n",
       " 'cast',\n",
       " 'contains',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'endswith',\n",
       " 'eqNullSafe',\n",
       " 'getField',\n",
       " 'getItem',\n",
       " 'isNotNull',\n",
       " 'isNull',\n",
       " 'isin',\n",
       " 'like',\n",
       " 'name',\n",
       " 'otherwise',\n",
       " 'over',\n",
       " 'rlike',\n",
       " 'startswith',\n",
       " 'substr',\n",
       " 'when']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(df.col1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Rounding Functions](http://www.legendu.net/misc/blog/spark-dataframe-func-rounding)\n",
    "\n",
    "Please refer to \n",
    "[Rounding Functions in Spark](http://www.legendu.net/misc/blog/spark-dataframe-func-rounding)\n",
    "for details.\n",
    "\n",
    "## [String Functions](http://www.legendu.net/misc/blog/spark-dataframe-func-string)\n",
    "\n",
    "Please refer to \n",
    "[String Functions in Spark](http://www.legendu.net/misc/blog/spark-dataframe-func-string)\n",
    "for details.\n",
    "\n",
    "## [Statistical Functions](http://www.legendu.net/misc/blog/spark-stat-functions)\n",
    "\n",
    "Please refer to\n",
    "[Statistical Functions in Spark](http://www.legendu.net/misc/blog/spark-stat-functions)\n",
    "for details.\n",
    "\n",
    "## [Date Functions in Spark](http://www.legendu.net/misc/blog/spark-dataframe-func-date)\n",
    "\n",
    "Please refer to \n",
    "[Date Functions in Spark](http://www.legendu.net/misc/blog/spark-dataframe-func-date)\n",
    "for details.\n",
    "\n",
    "## [Window Functions in Spark](http://www.legendu.net/misc/blog/window-functions-in-spark)\n",
    "\n",
    "Please refer to \n",
    "[Window Functions in Spark](http://www.legendu.net/misc/blog/window-functions-in-spark)\n",
    "for details.\n",
    "\n",
    "## [Collection Functions](http://www.legendu.net/misc/blog/spark-dataframe-func-collection)\n",
    "\n",
    "Please refer to\n",
    "[Collection Functions](http://www.legendu.net/misc/blog/spark-dataframe-func-collection)\n",
    "for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not (`~`) for Column Expressions\n",
    "\n",
    "Use `~` to reverse the values of a boolean column expression.\n",
    "Notice that you cannot use the `not` keyword in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+-----+\n",
      "|col1|col2|col3|col4| col5|\n",
      "+----+----+----+----+-----+\n",
      "|   3|   c| foo| 5.0|false|\n",
      "|   4|   d| bar| 7.0|false|\n",
      "+----+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~col(\"col5\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+-----------+\n",
      "|  col1|col2|col3|  hash_code|\n",
      "+------+----+----+-----------+\n",
      "|[1, 2]| how|   1|-1205091763|\n",
      "|[2, 3]| are|   2| -422146862|\n",
      "|[3, 4]| you|   3| -315368575|\n",
      "+------+----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"hash_code\", hash(\"col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when\n",
    "\n",
    "1. `null` in when condition is considered as false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = spark.read.json(\"../data/people.json\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`null` in when condition is considered as `false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|gt20|\n",
      "+----+\n",
      "|   0|\n",
      "|   1|\n",
      "|   0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(when($\"age\" > 20, 1).otherwise(0).alias(\"gt20\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|le20|\n",
      "+----+\n",
      "|   0|\n",
      "|   0|\n",
      "|   1|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(when($\"age\" <= 20, 1).otherwise(0).alias(\"le20\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  0|\n",
      "|100|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(when($\"age\".isNull, 0).when($\"age\" > 20 , 100).otherwise(10).alias(\"age\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|   0|\n",
      "|null|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(when($\"age\".isNull, 0).alias(\"age\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
