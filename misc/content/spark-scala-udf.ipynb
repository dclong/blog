{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: UDF in Spark\n",
    "- Slug: spark-scala-udf\n",
    "- Date: 2019-11-26\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, UDF, user-defined function\n",
    "- Author: Ben Du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map vs UDF\n",
    "\n",
    "https://stackoverflow.com/questions/38860808/performance-impact-of-rdd-api-vs-udfs-mixed-with-dataframe-api\n",
    "\n",
    "https://stackoverflow.com/questions/39039081/difference-between-a-map-and-udf\n",
    "\n",
    "https://stackoverflow.com/questions/43411234/spark-sql-whether-to-use-row-transformation-or-udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-udfs.html\n",
    "\n",
    "https://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/\n",
    "\n",
    "Use the higher-level standard Column-based functions with Dataset operators \n",
    "whenever possible before reverting to using your own custom UDF functions \n",
    "since UDFs are a blackbox for Spark and so it does not even try to optimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ed5710-e842-4068-9cdd-8788f8991454",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.11 2.3.1\n",
    "org.apache.spark spark-sql_2.11 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@2269a270"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Spark UDF Examples\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| text|\n",
      "+---+-----+\n",
      "|  0|hello|\n",
      "|  1|world|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (0, \"hello\"), \n",
    "    (1, \"world\")\n",
    ").toDF(\"id\", \"text\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,StringType,Some(List(StringType)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val upper: String => String = _.toUpperCase\n",
    "val upperUDF = udf(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| text|upper|\n",
      "+---+-----+-----+\n",
      "|  0|hello|HELLO|\n",
      "|  1|world|WORLD|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"upper\", upperUDF($\"text\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function2>,LongType,Some(List(LongType, LongType)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val someUDF = udf((arg1: Long, arg2: Long) => {\n",
    "    arg1 + arg2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
