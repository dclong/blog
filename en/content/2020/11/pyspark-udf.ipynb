{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: User-defined Function (UDF) in PySpark\n",
    "- Slug: pyspark-udf\n",
    "- Date: 2020-11-03 15:43:52\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Python, HPC, high performance computing, PySpark, UDF, pandas, pandas_udf, pandas UDF\n",
    "- Author: Ben Du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. Pandas UDFs are preferred to UDFs\n",
    "    as Pandas UDFs are typically much faster than UDFs.\n",
    "    \n",
    "1. The easist way to define a UDF in PySpark is to use the `@udf` tag,\n",
    "    and similarly the easist way to define a Pandas UDF in PySpark is to use the `@pandas_udf` tag.\n",
    "\n",
    "2. You need to specify the return type of the UDF, \n",
    "    e.g., `StringType()`. \n",
    "    Notice that you can use the string version of the return type,\n",
    "    e.g., `\"string\"` for `StringType()`.\n",
    "    The string version is simpler to use as you do not have to import the corresponding types\n",
    "    and string versions are short to type. \n",
    "    However, \n",
    "    it is at a slight cost of losing the ability to do static type checking (e.g., using pylint) on the used return types. \n",
    "    \n",
    "3. An UDF can take multiple columns as parameters.\n",
    "\n",
    "4. When invoking a UDF, \n",
    "    you can either pass column expression (e.g., `col(\"name\")`)\n",
    "    or the name of the column (e.g., `\"name\"`) directly to it.\n",
    "    It is suggested that you pass column names to an UDF\n",
    "    as it is simple and passing `col(\"name\")` requires the column name anyway.\n",
    "    \n",
    "5. BinaryType has already been supported in versions earlier than Spark 2.4. \n",
    "    However, \n",
    "    conversion between a Spark DataFrame which contains BinaryType columns \n",
    "    and a pandas DataFrame (via pyarrow) is not supported until spark 2.4.\n",
    "    \n",
    "6. Pandas UDF leveraging PyArrow (>=0.15) causes `java.lang.IllegalArgumentException` in PySpark 2.4 \n",
    "    (PySpark 3 has fixed issues completely). \n",
    "    Listed below are 3 ways to fix this issue. \n",
    "    For more discussions please refer to \n",
    "    [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html),\n",
    "    [PySpark pandas_udfs java.lang.IllegalArgumentException error](https://www.nuomiphp.com/eplan/en/52878.html)\n",
    "    and\n",
    "    [pandas udf not working with latest pyarrow release (0.15.0)](https://issues.apache.org/jira/browse/SPARK-29367)\n",
    "    .\n",
    "    \n",
    "    1. Downgrade PyArrow to 0.14.1 (if you have to stick to PySpark 2.4).\n",
    "    2. Set the environment variable `ARROW_PRE_0_15_IPC_FORMAT` to be `1` (if you have to stick to PySpark 2.4).\n",
    "        You can do this using `spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT=1`\n",
    "        and `spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT=1`.\n",
    "    3. Use PySpark 3.\n",
    "    \n",
    "7. You cannot directly pass a constant value to a UDF or pandas UDF. \n",
    "    However, \n",
    "    you can this using curried functions. \n",
    "    Below is an example taken from the StackOverflow question \n",
    "    [How to pass a constant value to Python UDF?](https://stackoverflow.com/questions/35375255/how-to-pass-a-constant-value-to-python-udf).\n",
    "        def comparatorUDF(n):\n",
    "            return udf(lambda c: c == n, BooleanType())\n",
    "\n",
    "        df.where(comparatorUDF(\"Bonsanto\")(col(\"name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.0.0-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark UDF\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|name| id|age|\n",
      "+----+---+---+\n",
      "| Ben|  2| 30|\n",
      "| Dan|  4| 25|\n",
      "|Will|  1| 26|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_p = pd.DataFrame(data=[\n",
    "    [\"Ben\", 2, 30],\n",
    "    [\"Dan\", 4, 25],\n",
    "    [\"Will\", 1, 26],\n",
    "], columns=[\"name\", \"id\", \"age\"])\n",
    "df = spark.createDataFrame(df_p)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF \n",
    "\n",
    "BinaryType has already been supported in versions earlier than Spark 2.4. \n",
    "However, \n",
    "conversion between a Spark DataFrame which contains BinaryType columns \n",
    "and a pandas DataFrame (via pyarrow) is not supported until spark 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def age_plus_one(age: pd.Series) -> pd.Series:\n",
    "     return age + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----+\n",
      "|name| id|age|age2|\n",
      "+----+---+---+----+\n",
      "| Ben|  2| 30|  31|\n",
      "| Dan|  4| 25|  26|\n",
      "|Will|  1| 26|  27|\n",
      "+----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age2\", age_plus_one(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def age_plus_one(age: pd.Series) -> pd.Series:\n",
    "     return pd.Series(a + 1 for a in age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----+\n",
      "|name| id|age|age2|\n",
      "+----+---+---+----+\n",
      "| Ben|  2| 30|  31|\n",
      "| Dan|  4| 25|  26|\n",
      "|Will|  1| 26|  27|\n",
      "+----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age2\", age_plus_one(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def concat(name: pd.Series, age: pd.Series) -> pd.Series:\n",
    "     return name + \" is \" + \"age years old.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------------------+\n",
      "|name| id|age|               intro|\n",
      "+----+---+---+--------------------+\n",
      "| Ben|  2| 30|Ben is age years ...|\n",
      "| Dan|  4| 25|Dan is age years ...|\n",
      "|Will|  1| 26|Will is age years...|\n",
      "+----+---+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"intro\", concat(\"name\", \"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def concat2(name: pd.Series, age: pd.Series) -> pd.Series:\n",
    "     return pd.Series(f\"{name} is {age} years old.\" for name, age in zip(name, age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------------------+\n",
      "|name| id|age|               intro|\n",
      "+----+---+---+--------------------+\n",
      "| Ben|  2| 30|Ben is 30 years old.|\n",
      "| Dan|  4| 25|Dan is 25 years old.|\n",
      "|Will|  1| 26|Will is 26 years ...|\n",
      "+----+---+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"intro\", concat2(\"name\", \"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Taking One Column as Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def say_hello(name: str) -> str:\n",
    "     return f\"Hello {name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----------+\n",
      "|name| id|age| greetings|\n",
      "+----+---+---+----------+\n",
      "| Ben|  2| 30| Hello Ben|\n",
      "| Dan|  4| 25| Hello Dan|\n",
      "|Will|  1| 26|Hello Will|\n",
      "+----+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"greetings\", say_hello(col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Taking Two Columns as Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\")\n",
    "def concat(name: str, age: int) -> str:\n",
    "     return f\"{name} is {age} years old.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------------------+\n",
      "|name| id|age|           greetings|\n",
      "+----+---+---+--------------------+\n",
      "| Ben|  2| 30|Ben is 30 years old.|\n",
      "| Dan|  4| 25|Dan is 25 years old.|\n",
      "|Will|  1| 26|Will is 26 years ...|\n",
      "+----+---+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"greetings\", concat(col(\"name\"), col(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+--------------------+\n",
      "|name| id|age|           greetings|\n",
      "+----+---+---+--------------------+\n",
      "| Ben|  2| 30|Ben is 30 years old.|\n",
      "| Dan|  4| 25|Dan is 25 years old.|\n",
      "|Will|  1| 26|Will is 26 years ...|\n",
      "+----+---+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"greetings\", concat(\"name\", \"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Scalable Python Code with Pandas UDFs: A Data Science Application](https://towardsdatascience.com/scalable-python-code-with-pandas-udfs-a-data-science-application-dd515a628896)\n",
    "\n",
    "https://medium.com/@ayplam/developing-pyspark-udfs-d179db0ccc87\n",
    "\n",
    "https://medium.com/analytics-vidhya/pyspark-udf-deep-dive-8ae984bfac00\n",
    "    \n",
    "https://docs.databricks.com/spark/latest/spark-sql/udf-python.html\n",
    "    \n",
    "https://changhsinlee.com/pyspark-udf/\n",
    "    \n",
    "https://medium.com/@ayplam/developing-pyspark-udfs-d179db0ccc87\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
